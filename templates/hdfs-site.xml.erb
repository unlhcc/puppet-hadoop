<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<!-- ##################################################################### -->
<!-- #                                                                   # -->
<!-- #         DO NOT EDIT - file is being maintained by puppet          # -->
<!-- #                                                                   # -->
<!-- ##################################################################### -->

<% if has_variable?("hdfs_dfsdir_list") then -%>
<!-- hdfs_dfsdir_list = <%= @hdfs_dfsdir_list %> -->
<% else %>
<!-- WARNING: hdfs_dfsdir_list fact not set -->
<!-- (if this isn't a datanode, then it's ok) -->
<% end %>

<configuration>
<property>
  <name>dfs.permissions.superusergroup</name>
  <value>root</value>
</property>
<property>
  <name>dfs.namenode.replication.max</name>
  <value>10</value>
</property>
<property>
  <name>dfs.datanode.du.reserved</name>
  <value>10000000000</value>
</property>
<property>
  <name>dfs.datanode.balance.bandwidthPerSec</name>
  <value>2000000000</value>
</property>
<property>
  <name>dfs.datanode.data.dir</name>
  <value><% if has_variable?("hdfs_dfsdir_list") then -%><%= @hdfs_dfsdir_list %><% else %>/hadoop-data1<% end %></value>
</property>
<property>
  <name>dfs.datanode.failed.volumes.tolerated</name>
  <value>1</value>
</property>
<property>
  <name>dfs.datanode.handler.count</name>
  <value>10</value>
</property>
<property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>4096</value>
</property>
<property>
  <name>dfs.hosts.exclude</name>
  <value>/etc/hadoop/conf/hosts_exclude</value>
</property>
<property>
  <name>dfs.namenode.handler.count</name>
  <value>40</value>
</property>
<property>
  <name>dfs.namenode.logging.level</name>
  <value>all</value>
</property>
<property>
  <name>fs.namenode.checkpoint.dir</name>
  <value>/hadoop-data1/checkpoints,/mnt/fsimage/current</value>
</property>
<property>
  <name>dfs.image.transfer.timeout</name>
  <value>600000</value>
</property>
<property>
  <name>net.topology.script.file.name</name>
  <value>/etc/hadoop/conf/rackmap.pl</value>
</property>
<property>
  <name>dfs.namenode.secondary.http.address</name>
  <value>hadoop-tracker:50090</value>
  <description>
    The secondary namenode http server address and port.
    If the port is 0 then the server will start on a free port.
  </description>
</property>
<property>
  <name>dfs.namenode.http.address</name>
  <value>hadoop-name:50070</value>
  <description>
    The address and the base port where the dfs namenode web ui will listen on.
    If the port is 0 then the server will start on a free port.
  </description>
</property>
<property>
  <name>fs.namenode.checkpoint.period</name>
  <value>900</value>
  <description>The number of seconds between two periodic checkpoints.
  </description>
</property>
<property>
  <name>dfs.namenode.replication.work.multiplier.per.iteration</name>
  <value>10</value>
</property>
<property>
  <name>dfs.namenode.replication.max-streams</name>
  <value>10</value>
</property>
<property>
  <name>dfs.namenode.replication.max-streams-hard-limit</name>
  <value>15</value>
</property>
<property>
  <name>dfs.image.transfer.bandwidthPerSec</name>
  <value>26214400</value>
</property>
<property>
  <name>dfs.disk.balancer.enabled</name>
  <value>true</value>
</property>
<property>
  <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
  <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
</property>
<property>
  <name>dfs.datanode.sync.behind.writes</name>
  <value>true</value>
</property>
<property>
  <name>dfs.datanode.synconclose</name>
  <value>true</value>
</property>
<property>
  <name>dfs.blocksize</name>
  <value>536870912</value>
</property>
</configuration>
